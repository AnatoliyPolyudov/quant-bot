# train_model.py
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.utils import class_weight
import joblib
import os
import matplotlib.pyplot as plt
import seaborn as sns

def load_training_data():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–æ–π"""
    data_file = "data/training_data.csv"
    
    if not os.path.exists(data_file):
        print("‚ùå –§–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–Ω–∞—á–∞–ª–∞ —Å–æ–±–µ—Ä–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ.")
        return None
    
    try:
        df = pd.read_csv(data_file)
        print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–æ–ª–æ–Ω–∫–∏ target
        if 'target' not in df.columns:
            print("‚ùå –ö–æ–ª–æ–Ω–∫–∞ 'target' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –¥–∞–Ω–Ω—ã—Ö")
            return None
            
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        target_data = df['target'].dropna()
        if len(target_data) == 0:
            print("‚ùå –ù–µ—Ç —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (target). –ü—Ä–æ–¥–æ–ª–∂–∞–π—Ç–µ —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö.")
            return None
        
        print(f"üéØ –†–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π: {len(target_data)}")
        
        return df
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
        return None

def detailed_data_analysis(df):
    """–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö"""
    print("\nüîç –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–•:")
    
    # –£–±–∏—Ä–∞–µ–º —Å—Ç—Ä–æ–∫–∏ –±–µ–∑ target
    df_labeled = df.dropna(subset=['target'])
    df_labeled['target'] = df_labeled['target'].astype(int)
    
    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ target
    target_counts = df_labeled['target'].value_counts().sort_index()
    total_labeled = len(df_labeled)
    
    print(f"üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ target ({total_labeled} –∑–∞–ø–∏—Å–µ–π):")
    for target_val in [-1, 0, 1]:
        count = target_counts.get(target_val, 0)
        percentage = count / total_labeled * 100
        symbol = "üî¥" if target_val == -1 else "‚ö™" if target_val == 0 else "üü¢"
        print(f"   {symbol} Target {target_val}: {count} –∑–∞–ø–∏—Å–µ–π ({percentage:.1f}%)")
    
    # –ê–Ω–∞–ª–∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    print(f"\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
    feature_columns = ['order_book_imbalance', 'spread_percent', 'cumulative_delta', 
                      'funding_rate', 'volatility']
    
    for feature in feature_columns:
        if feature in df_labeled.columns:
            stats = df_labeled[feature].describe()
            print(f"   {feature}:")
            print(f"      min={stats['min']:.6f}, max={stats['max']:.6f}")
            print(f"      mean={stats['mean']:.6f}, std={stats['std']:.6f}")
            print(f"      non-zero: {(df_labeled[feature] != 0).sum()}/{len(df_labeled)}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ—Å—Ç–æ—è–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
    constant_features = []
    for feature in feature_columns:
        if feature in df_labeled.columns and df_labeled[feature].nunique() <= 1:
            constant_features.append(feature)
    
    if constant_features:
        print(f"‚ö†Ô∏è  –ü–æ—Å—Ç–æ—è–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {constant_features}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ —Å target
    print(f"\nüìä –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å target:")
    for feature in feature_columns:
        if feature in df_labeled.columns:
            correlation = df_labeled[feature].corr(df_labeled['target'])
            print(f"   {feature}: {correlation:.3f}")
    
    return df_labeled

def create_baseline_model(df):
    """–°–æ–∑–¥–∞–µ—Ç –±–µ–π–∑–ª–∞–π–Ω –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ—Å—Ç—ã—Ö –ø—Ä–∞–≤–∏–ª"""
    print("\nü§ñ –ë–ï–ô–ó–õ–ê–ô–ù –ú–û–î–ï–õ–¨ (–ø—Ä–∞–≤–∏–ª–∞):")
    
    # –ü—Ä–∞–≤–∏–ª–æ 1: Imbalance
    df['baseline_imbalance'] = (df['order_book_imbalance'] > 0.6).astype(int)
    accuracy_imbalance = accuracy_score(df['target'] == 1, df['baseline_imbalance'])
    print(f"üìä Imbalance > 0.6 accuracy: {accuracy_imbalance:.3f}")
    
    # –ü—Ä–∞–≤–∏–ª–æ 2: Delta
    df['baseline_delta'] = (df['cumulative_delta'] > 0).astype(int)
    accuracy_delta = accuracy_score(df['target'] == 1, df['baseline_delta'])
    print(f"üìà Delta > 0 accuracy: {accuracy_delta:.3f}")
    
    # –ü—Ä–∞–≤–∏–ª–æ 3: Combined
    df['baseline_combined'] = ((df['order_book_imbalance'] > 0.6) & 
                              (df['cumulative_delta'] > 0)).astype(int)
    accuracy_combined = accuracy_score(df['target'] == 1, df['baseline_combined'])
    print(f"üéØ Combined rule accuracy: {accuracy_combined:.3f}")
    
    # –ë–µ–π–∑–ª–∞–π–Ω –¥–ª—è –≤—Å–µ—Ö –∫–ª–∞—Å—Å–æ–≤
    baseline_majority = (df['target'] == 0).mean()
    print(f"üìä Majority class (HOLD) accuracy: {baseline_majority:.3f}")

def handle_class_imbalance(df):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤"""
    print("\n‚öñÔ∏è  –û–ë–†–ê–ë–û–¢–ö–ê –î–ò–°–ë–ê–õ–ê–ù–°–ê –ö–õ–ê–°–°–û–í:")
    
    target_counts = df['target'].value_counts()
    print(f"–ò—Å—Ö–æ–¥–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {target_counts.to_dict()}")
    
    # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤
    class_weights = class_weight.compute_class_weight(
        class_weight='balanced',
        classes=np.array([-1, 0, 1]),
        y=df['target']
    )
    
    weight_dict = {-1: class_weights[0], 0: class_weights[1], 1: class_weights[2]}
    print(f"–í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤: {weight_dict}")
    
    return weight_dict

def train_ml_model(df):
    """–û–±—É—á–∞–µ—Ç ML –º–æ–¥–µ–ª—å —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
    print("\nüß† –û–ë–£–ß–ï–ù–ò–ï ML –ú–û–î–ï–õ–ò...")
    
    # –ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –º–æ–¥–µ–ª–∏
    feature_columns = [
        'order_book_imbalance',
        'spread_percent', 
        'cumulative_delta',
        'funding_rate',
        'buy_trades',
        'sell_trades', 
        'total_trades',
        'volatility'
    ]
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    missing_features = [f for f in feature_columns if f not in df.columns]
    if missing_features:
        print(f"‚ö†Ô∏è  –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏: {missing_features}")
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        feature_columns = [f for f in feature_columns if f in df.columns]
    
    X = df[feature_columns]
    y = df['target']
    
    # –£–±–∏—Ä–∞–µ–º NaN
    mask = ~X.isna().any(axis=1)
    X = X[mask]
    y = y[mask]
    
    print(f"üìä –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(X)} –∑–∞–ø–∏—Å–µ–π")
    print(f"üéØ –ü—Ä–∏–∑–Ω–∞–∫–∏: {feature_columns}")
    
    if len(X) < 50:
        print(f"‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –ù—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 50, —Å–µ–π—á–∞—Å: {len(X)}")
        return None
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤
    class_weights = handle_class_imbalance(pd.DataFrame({'target': y}))
    
    # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —Ä—è–¥–∞–º–∏
    tscv = TimeSeriesSplit(n_splits=min(5, len(X) // 10))
    model = RandomForestClassifier(
        n_estimators=100,
        random_state=42,
        class_weight=class_weights,
        max_depth=10,
        min_samples_split=5
    )
    
    # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
    print("üìä –ó–∞–ø—É—Å–∫ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏...")
    scores = cross_val_score(model, X, y, cv=tscv, scoring='accuracy')
    print(f"üìä –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è accuracy: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})")
    
    # –û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
    model.fit(X, y)
    print("‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞!")
    
    # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    feature_importance = pd.DataFrame({
        'feature': feature_columns,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print("\nüéØ –í–ê–ñ–ù–û–°–¢–¨ –ü–†–ò–ó–ù–ê–ö–û–í:")
    for _, row in feature_importance.iterrows():
        print(f"  {row['feature']}: {row['importance']:.3f}")
    
    # –î–µ—Ç–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
    y_pred = model.predict(X)
    print(f"\nüìà –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö: {accuracy_score(y, y_pred):.3f}")
    
    # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
    cm = confusion_matrix(y, y_pred)
    print(f"üìä –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫:")
    print(f"   True -1: {cm[0]}")
    print(f"   True  0: {cm[1]}") 
    print(f"   True  1: {cm[2]}")
    
    return model, feature_columns

def save_model(model, feature_columns, filename="models/quant_model.pkl"):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ"""
    os.makedirs("models", exist_ok=True)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    joblib.dump(model, filename)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    metadata = {
        'feature_columns': feature_columns,
        'model_type': 'RandomForest',
        'timestamp': pd.Timestamp.now().strftime("%Y-%m-%d %H:%M:%S"),
        'classes': model.classes_.tolist() if hasattr(model, 'classes_') else []
    }
    joblib.dump(metadata, "models/model_metadata.pkl")
    
    print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {filename}")
    print(f"üìù –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: models/model_metadata.pkl")
    print(f"üéØ –ö–ª–∞—Å—Å—ã –º–æ–¥–µ–ª–∏: {metadata['classes']}")

def plot_feature_importance(model, feature_columns):
    """–í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    try:
        importance_df = pd.DataFrame({
            'feature': feature_columns,
            'importance': model.feature_importances_
        }).sort_values('importance', ascending=True)
        
        plt.figure(figsize=(10, 6))
        plt.barh(importance_df['feature'], importance_df['importance'])
        plt.xlabel('–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∞')
        plt.title('–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ –º–æ–¥–µ–ª–∏')
        plt.tight_layout()
        plt.savefig('models/feature_importance.png', dpi=300, bbox_inches='tight')
        print("üìä –ì—Ä–∞—Ñ–∏–∫ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: models/feature_importance.png")
        plt.close()
    except Exception as e:
        print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫: {e}")

def main():
    print("üöÄ –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø –ú–û–î–ï–õ–ò...")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    df = load_training_data()
    if df is None:
        return
    
    # –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
    df_labeled = detailed_data_analysis(df)
    if df_labeled is None or len(df_labeled) == 0:
        print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        return
    
    # –ë–µ–π–∑–ª–∞–π–Ω –º–æ–¥–µ–ª—å
    create_baseline_model(df_labeled)
    
    # ML –º–æ–¥–µ–ª—å (–µ—Å–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö)
    if len(df_labeled) >= 30:
        print(f"\nüîß –û–±—É—á–µ–Ω–∏–µ –Ω–∞ {len(df_labeled)} –∑–∞–ø–∏—Å—è—Ö...")
        result = train_ml_model(df_labeled)
        if result:
            model, feature_columns = result
            save_model(model, feature_columns)
            plot_feature_importance(model, feature_columns)
            print("\nüéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é.")
        else:
            print("‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏")
    else:
        print(f"üìä –ü—Ä–æ–¥–æ–ª–∂–∞–π—Ç–µ —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö. –°–µ–π—á–∞—Å: {len(df_labeled)}/30")

if __name__ == "__main__":
    main()
