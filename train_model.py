# train_model.py
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from sklearn.metrics import classification_report, accuracy_score
import joblib
import os

def load_training_data():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    data_file = "data/training_data.csv"
    
    if not os.path.exists(data_file):
        print("‚ùå –§–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–Ω–∞—á–∞–ª–∞ —Å–æ–±–µ—Ä–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ.")
        return None
    
    df = pd.read_csv(data_file)
    print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ target
    if 'target' not in df.columns or df['target'].isna().all():
        print("‚ùå –ù–µ—Ç —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (target). –ü—Ä–æ–¥–æ–ª–∂–∞–π—Ç–µ —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö.")
        return None
    
    # –£–±–∏—Ä–∞–µ–º —Å—Ç—Ä–æ–∫–∏ –±–µ–∑ target
    df = df.dropna(subset=['target'])
    df['target'] = df['target'].astype(int)
    
    print(f"üéØ –†–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π: {len(df)}")
    print(f"üìà –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ target: {df['target'].value_counts().to_dict()}")
    
    return df

def create_baseline_model(df):
    """–°–æ–∑–¥–∞–µ—Ç –±–µ–π–∑–ª–∞–π–Ω –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ—Å—Ç—ã—Ö –ø—Ä–∞–≤–∏–ª"""
    print("\nü§ñ –ë–ï–ô–ó–õ–ê–ô–ù –ú–û–î–ï–õ–¨ (–ø—Ä–∞–≤–∏–ª–∞):")
    
    # –ü—Ä–∞–≤–∏–ª–æ 1: Imbalance > 0.6 = –ø–æ–∫—É–ø–∞—Ç—å
    df['baseline_imbalance'] = (df['order_book_imbalance'] > 0.6).astype(int)
    accuracy_imbalance = accuracy_score(df['target'] == 1, df['baseline_imbalance'])
    print(f"üìä Imbalance > 0.6 accuracy: {accuracy_imbalance:.3f}")
    
    # –ü—Ä–∞–≤–∏–ª–æ 2: Delta > 0 = –ø–æ–∫—É–ø–∞—Ç—å
    df['baseline_delta'] = (df['cumulative_delta'] > 0).astype(int)
    accuracy_delta = accuracy_score(df['target'] == 1, df['baseline_delta'])
    print(f"üìà Delta > 0 accuracy: {accuracy_delta:.3f}")
    
    # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø—Ä–∞–≤–∏–ª–æ
    df['baseline_combined'] = ((df['order_book_imbalance'] > 0.6) & 
                              (df['cumulative_delta'] > 0)).astype(int)
    accuracy_combined = accuracy_score(df['target'] == 1, df['baseline_combined'])
    print(f"üéØ Combined rule accuracy: {accuracy_combined:.3f}")

def train_ml_model(df):
    """–û–±—É—á–∞–µ—Ç ML –º–æ–¥–µ–ª—å"""
    print("\nüß† –û–ë–£–ß–ï–ù–ò–ï ML –ú–û–î–ï–õ–ò...")
    
    # –ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –º–æ–¥–µ–ª–∏
    feature_columns = [
        'order_book_imbalance',
        'spread_percent', 
        'cumulative_delta',
        'funding_rate',
        'buy_trades',
        'sell_trades',
        'total_trades'
    ]
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    missing_features = [f for f in feature_columns if f not in df.columns]
    if missing_features:
        print(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏: {missing_features}")
        return None
    
    X = df[feature_columns]
    y = df['target']
    
    # –£–±–∏—Ä–∞–µ–º NaN
    mask = ~X.isna().any(axis=1)
    X = X[mask]
    y = y[mask]
    
    print(f"üìä –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(X)} –∑–∞–ø–∏—Å–µ–π")
    
    # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —Ä—è–¥–∞–º–∏
    tscv = TimeSeriesSplit(n_splits=5)
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    
    # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
    scores = cross_val_score(model, X, y, cv=tscv, scoring='accuracy')
    print(f"üìä –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è accuracy: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})")
    
    # –û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
    model.fit(X, y)
    print("‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞!")
    
    # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    feature_importance = pd.DataFrame({
        'feature': feature_columns,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print("\nüéØ –í–ê–ñ–ù–û–°–¢–¨ –ü–†–ò–ó–ù–ê–ö–û–í:")
    for _, row in feature_importance.iterrows():
        print(f"  {row['feature']}: {row['importance']:.3f}")
    
    return model

def save_model(model, filename="models/quant_model.pkl"):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å"""
    os.makedirs("models", exist_ok=True)
    joblib.dump(model, filename)
    print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {filename}")

def main():
    print("üöÄ –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø –ú–û–î–ï–õ–ò...")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    df = load_training_data()
    if df is None or len(df) < 50:
        print(f"‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –ù—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 50 –∑–∞–ø–∏—Å–µ–π, —Å–µ–π—á–∞—Å: {len(df) if df is not None else 0}")
        return
    
    # –ë–µ–π–∑–ª–∞–π–Ω –º–æ–¥–µ–ª—å
    create_baseline_model(df)
    
    # ML –º–æ–¥–µ–ª—å (–µ—Å–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö)
    if len(df) >= 50:
        model = train_ml_model(df)
        if model:
            save_model(model)
            print("\nüéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é.")
    else:
        print(f"üìä –ü—Ä–æ–¥–æ–ª–∂–∞–π—Ç–µ —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö. –°–µ–π—á–∞—Å: {len(df)}/50")

if __name__ == "__main__":
    main()
